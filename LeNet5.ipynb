{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ca80f1c",
   "metadata": {},
   "source": [
    "# Overall Description:\n",
    "The code loads the MNIST dataset, which contains images of handwritten digits, and performs necessary preprocessing steps. It normalizes the pixel values of the training and test images to a range of 0-1. Then, it expands the dimensions of the input images to match the expected shape of convolutional neural networks. A subset of the training data is selected as the validation set.\n",
    "\n",
    "The code defines the LeNet-5 model architecture using the Keras Sequential API. The model consists of convolutional layers with tanh activation, average pooling layers, a flatten layer, and dense layers with tanh activation. The output layer uses the softmax activation to produce class probabilities. The model is compiled with the Adam optimizer, sparse categorical cross-entropy loss, and accuracy as the evaluation metric.\n",
    "\n",
    "The model is trained for 5 epochs using the training data, with the validation data used for monitoring the model's performance. After training, the model is evaluated on the test data using the evaluate() method, providing insights into its performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89dd7474",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86212fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the MNIST dataset using the `load_data()` function from the `keras.datasets.mnist` module. \n",
    "# The MNIST dataset is a widely used dataset in machine learning, consisting of 60,000 training images and 10,000 test images of handwritten digits (0-9).\n",
    "# The training set is split into `train_x` (input images) and `train_y` (output labels), while the test set is split into `test_x` and `test_y`.\n",
    "(train_x, train_y), (test_x, test_y) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalizing the training input images by dividing each pixel value by 255.0\n",
    "# This step scales down the pixel values from the original range of 0-255 to a normalized range of 0-1, \n",
    "# which can help improve the training process for neural networks.\n",
    "train_x = train_x / 255.0\n",
    "test_x = test_x / 255.0\n",
    "\n",
    "\n",
    "# Expanding the dimensions of the training input images using the `tf.expand_dims()` function from TensorFlow. \n",
    "# The `train_x` tensor has shape `(num_samples, width, height)`, and this line adds an extra dimension at the end with size 1, resulting in a shape of \n",
    "# `(num_samples, width, height, 1)`.\n",
    "# The additional dimension is added to match the expected input shape of convolutional neural networks (CNNs), \n",
    "# which typically take inputs in the form of `(batch_size, width, height, channels)`. \n",
    "# In this case, the `channels` dimension is set to 1 since MNIST images are grayscale.\n",
    "train_x = tf.expand_dims(train_x, 3)\n",
    "test_x = tf.expand_dims(test_x, 3)\n",
    "\n",
    "# Creating a validation set `val_x` by taking the first 5000 samples from the expanded training input images `train_x`. \n",
    "# Creates the corresponding validation output labels `val_y` by taking the first 5000 samples from the original training output labels `train_y`. \n",
    "val_x = train_x[:5000]\n",
    "val_y = train_y[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb3897bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a sequential model using keras.models.Sequential(). \n",
    "# A sequential model is a linear stack of layers, where the output of one layer becomes the input of the next layer.\n",
    "lenet_5_model = keras.models.Sequential([\n",
    "    \n",
    "# This line adds a convolutional layer to the model. It uses Conv2D class from Keras. \n",
    "# The arguments passed to the Conv2D layer:\n",
    "# The number of filters in the convolutional layer = 6. Each filter learns different patterns in the input image.\n",
    "#  kernel_size=5. The size of the convolutional kernel/filter is set to 5x5.\n",
    "# strides=1. The stride of the kernel is set to 1, meaning the kernel moves one pixel at a time during convolution.\n",
    "# activation='tanh' . The activation function used after the convolution operation.\n",
    "# input_shape=train_x[0].shape - This specifies the shape of the input to the first layer of the model. \n",
    "#     Since the input images have been expanded to include the channel dimension (height, width, channels), \n",
    "#     train_x[0].shape represents the shape of a single input image.\n",
    "# padding='same' - The padding is set to 'same', which means the input image is padded with zeros \n",
    "#     so that the output feature maps have the same spatial dimensions as the input.\n",
    "    keras.layers.Conv2D(6, kernel_size=5, strides=1,  activation='relu', input_shape=train_x[0].shape, padding='same'), #C1\n",
    "    \n",
    "# This line adds an average pooling layer to the model. \n",
    "#     Average pooling reduces the spatial dimensions of the input by taking the average value of each non-overlapping window. \n",
    "#     By default, the average pooling layer uses a 2x2 window with a stride of 2, which reduces the spatial dimensions by a factor of 2.\n",
    "    keras.layers.AveragePooling2D(), #S2\n",
    "    \n",
    "# Adding another convolutional layer to the model. It is similar to the first convolutional layer, but the number of filters is increased to 16. \n",
    "#     The padding argument is set to 'valid', which means no padding is applied to the input.\n",
    "    keras.layers.Conv2D(16, kernel_size=5, strides=1, activation='relu', padding='valid'), #C3\n",
    "    \n",
    "#This line adds another average pooling layer to the model, reducing the spatial dimensions further\n",
    "    keras.layers.AveragePooling2D(), #S4\n",
    "  \n",
    "# Adding a flatten layer to the model. The flatten layer reshapes the 2D output from the previous layer into a 1D vector. \n",
    "#     It effectively flattens the spatial dimensions into a single dimension while preserving the batch size\n",
    "    keras.layers.Flatten(), #Flatten\n",
    "    \n",
    "# This line adds a fully connected (dense) layer to the model. It consists of 120 neurons and applies the activation function ('tanh') to the outputs.\n",
    "    keras.layers.Dense(120, activation='relu'), #C5\n",
    "# This line adds another fully connected layer with 84 neurons and the hyperbolic tangent activation function.\n",
    "    keras.layers.Dense(84, activation='relu'), #F6\n",
    "# This line adds the output layer to the model. It consists of 10 neurons, corresponding to the 10 possible classes in the MNIST dataset (digits 0-9). \n",
    "# The activation function used is softmax, which produces a probability distribution over the classes, indicating the likelihood of each class.\n",
    "    keras.layers.Dense(10, activation='softmax') #Output layer\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94271c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiles the LeNet-5 model with the specified configuration for training:\n",
    "# The optimizer is set to 'adam'. Adam is an optimization algorithm that adapts the learning rate during training. \n",
    "# It combines the advantages of two other popular optimization algorithms, AdaGrad and RMSProp, \n",
    "# to provide efficient and effective optimization for neural networks.\n",
    "# The loss function is set to `sparse_categorical_crossentropy`. \n",
    "# This loss function is commonly used for multi-class classification problems, where the target labels are integers (as in the MNIST dataset). \n",
    "# It calculates the cross-entropy loss between the predicted class probabilities and the true class labels. \n",
    "# The use of `sparse_categorical_crossentropy` indicates that the true labels are provided as integers, rather than one-hot encoded vectors.\n",
    "# The model will compute and report the accuracy metric during training and evaluation. \n",
    "# Accuracy is a commonly used metric in classification tasks, indicating the proportion of correctly predicted labels compared to the total number of samples.\n",
    "lenet_5_model.compile(optimizer='adam', loss=keras.losses.sparse_categorical_crossentropy, metrics=['accuracy'])\n",
    "# By compiling the model with these settings, it is ready to be trained using the specified optimizer, loss function, and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16a59bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 20s 10ms/step - loss: 0.1937 - accuracy: 0.9424 - val_loss: 0.0762 - val_accuracy: 0.9788\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.0673 - accuracy: 0.9793 - val_loss: 0.0456 - val_accuracy: 0.9878\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.0468 - accuracy: 0.9856 - val_loss: 0.0295 - val_accuracy: 0.9914\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.0347 - accuracy: 0.9889 - val_loss: 0.0244 - val_accuracy: 0.9920\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 21s 11ms/step - loss: 0.0285 - accuracy: 0.9908 - val_loss: 0.0149 - val_accuracy: 0.9952\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a25a78b730>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# By calling fit() with these arguments, the LeNet-5 model will be trained using the provided training data and labels for the specified number of epochs. \n",
    "# The model's performance on both the training and validation data will be displayed, including the loss value and the specified metrics (in this case, accuracy).\n",
    "lenet_5_model.fit(train_x, train_y, epochs=5, validation_data=(val_x, val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5b2a28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 4ms/step - loss: 0.0571 - accuracy: 0.9830\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0570899099111557, 0.9829999804496765]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluating the performance of the trained LeNet-5 model on the test data\n",
    "lenet_5_model.evaluate(test_x, test_y)\n",
    "# The evaluation will include calculating the loss value and the specified metrics (in this case, accuracy) on the test data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
